#include <bang.h>

__mlu_global__ void mlu_layer_norm(
    float* input, 
    float* gamma, 
    float* beta, 
    float* output
) {
    __nram__ float diff[8];
    __nram__ float mean[8];
    __nram__ float variance[8];
    __nram__ float local_input[8];
    __nram__ float local_gamma[8];
    __nram__ float local_beta[8];
    __memcpy(local_gamma, gamma, 8, GDRAM2NRAM);
    __memcpy(local_beta, beta, 8, GDRAM2NRAM);

    if (clusterId  < 2) {
        if (coreId < 4) {
            __memcpy(local_input, input + clusterId * 4 * 8 + coreId * 8, 32, GDRAM2NRAM);
            __bang_sumpool(mean, local_input, 1, 1, 8, 1, 8, 1, 1, 1);
            __bang_mul(mean, mean, 1.0/8, 8);

            // Calculate variance
            __bang_sub(diff, local_input, mean, 8);
            __bang_mul(diff, diff, diff, 8);

            __bang_sumpool(variance, diff, 1, 1, 8, 1, 8, 1, 1, 1);
            variance[0] = sqrt(variance[0] / 8);

            // Normalize input
            __bang_sub(diff, local_input, mean, 8);
            __bang_mul(diff, diff, gamma, 8);
            __bang_div(diff, diff, variance[0] + 1e-5f, 8);
            __bang_add(diff, diff, beta, 8);
            __memcpy(output + clusterId * 4 * 8 + coreId * 8, diff, 32, NRAM2GDRAM);
        }
    }
}

extern "C" void layer_norm_kernel(float* input, float* gamma, float* beta, float* output) {
    cnrtQueue_t queue;
    cnrtSetDevice(0);
    cnrtQueueCreate(&queue);
    int batch_size = 2;
    int seq_length = 4;
    int d_model = 8;
    int num_elements = batch_size * seq_length * d_model;

    // Allocate memory on the device
    float* d_input;
    cnrtMalloc(&d_input, num_elements * sizeof(float));
    float* d_gamma;
    cnrtMalloc(&d_gamma, d_model * sizeof(float));
    float* d_beta;
    cnrtMalloc(&d_beta, d_model * sizeof(float));
    float* d_output;
    cnrtMalloc(&d_output, num_elements * sizeof(float));

    // Copy data from host to device
    cnrtMemcpy(d_input, input, num_elements * sizeof(float), cnrtMemcpyHostToDev);
    cnrtMemcpy(d_gamma, gamma, d_model * sizeof(float), cnrtMemcpyHostToDev);
    cnrtMemcpy(d_beta, beta, d_model * sizeof(float), cnrtMemcpyHostToDev);


    // Define the function type
    cnrtDim3_t dim = {1, 4, 4};
    cnrtFunctionType_t ktype = CNRT_FUNC_TYPE_UNION4;

    // Launch kernel
    mlu_layer_norm<<<dim, ktype, queue>>>(d_input, d_gamma, d_beta, d_output);

    // Copy the result back to host
    cnrtMemcpy(output, d_output, num_elements * sizeof(float), cnrtMemcpyDevToHost);

    // Free device memory
    cnrtFree(d_input);
    cnrtFree(d_gamma);
    cnrtFree(d_beta);
    cnrtFree(d_output);
}