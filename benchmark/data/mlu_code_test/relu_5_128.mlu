__mlu_global__ void relu(float *input0, float *active_relu_114) {
  __nram__ float input0_local_nram[128];
  if (((((int)clusterId) * 4) + ((int)coreId)) < 5) {
    __memcpy(((float *)input0_local_nram + (0)),
             ((float *)input0 +
              (((((int)clusterId) * 512) + (((int)coreId) * 128)))),
             512, GDRAM2NRAM);
  }
  if (((((int)clusterId) * 4) + ((int)coreId)) < 5) {
    __bang_active_relu(((float *)input0_local_nram + (0)),
                       ((float *)input0_local_nram + (0)), 128);
  }
  if (((((int)clusterId) * 4) + ((int)coreId)) < 5) {
    __memcpy(((float *)active_relu_114 +
              (((((int)clusterId) * 512) + (((int)coreId) * 128)))),
             ((float *)input0_local_nram + (0)), 512, NRAM2GDRAM);
  }
}

extern "C" void relu_kernel(float *C, float *A, int size) {
  float *d_A, *d_C;
  cnrtQueue_t queue;
  CNRT_CHECK(cnrtSetDevice(0));
  CNRT_CHECK(cnrtQueueCreate(&queue));
  cnrtMalloc((void **)(&d_A), size * sizeof(float));
  cnrtMalloc((void **)(&d_C), size * sizeof(float));

  cnrtMemcpy(d_A, A, size * sizeof(float), cnrtMemcpyHostToDev);

  cnrtDim3_t dim = {16, 1, 1};
  cnrtFunctionType_t ktype = CNRT_FUNC_TYPE_UNION4;

  relu<<<dim, ktype, queue>>>(d_A, d_C);

  cnrtMemcpy(C, d_C, size * sizeof(float), cnrtMemcpyDevToHost);

  cnrtFree(d_A);
  cnrtFree(d_C);
}
