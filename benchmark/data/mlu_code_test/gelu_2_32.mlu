__mlu_global__ void gelu(float *input0, float *active_gelup_80) {
  __nram__ float input0_local_nram[4];
  __memcpy(
      ((float *)input0_local_nram + (0)),
      ((float *)input0 + (((((int)clusterId) * 16) + (((int)coreId) * 4)))), 16,
      GDRAM2NRAM);
  __bang_active_gelup(((float *)input0_local_nram + (0)),
                      ((float *)input0_local_nram + (0)), 4);
  __memcpy(((float *)active_gelup_80 +
            (((((int)clusterId) * 16) + (((int)coreId) * 4)))),
           ((float *)input0_local_nram + (0)), 16, NRAM2GDRAM);
}

extern "C" void gelu_kernel(float *C, float *A, int size) {
  float *d_A, *d_C;
  cnrtQueue_t queue;
  CNRT_CHECK(cnrtSetDevice(0));
  CNRT_CHECK(cnrtQueueCreate(&queue));
  cnrtMalloc((void **)(&d_A), size * sizeof(float));
  cnrtMalloc((void **)(&d_C), size * sizeof(float));

  cnrtMemcpy(d_A, A, size * sizeof(float), cnrtMemcpyHostToDev);

  cnrtDim3_t dim = {16, 1, 1};
  cnrtFunctionType_t ktype = CNRT_FUNC_TYPE_UNION4;

  gelu<<<dim, ktype, queue>>>(d_A, d_C);

  cnrtMemcpy(C, d_C, size * sizeof(float), cnrtMemcpyDevToHost);

  cnrtFree(d_A);
  cnrtFree(d_C);
}
